{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FFr9TZzgkSB2",
        "dkDuhnN3raVq",
        "3beYvqzeuL6v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Nash equilibrium - GT\n",
        "GT stands for game theory\n",
        "\n",
        "This Jupyter Notebook provides a simple simulation of different strategies used to achieve nash equilibria and the corresponding Python code is the steps taken to achieve a personal conceptual understanding with the aim to employ these for reinforcement learning agent training.\n",
        "\n",
        "Welcome to the journey!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oElgicB1-mEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prisoner's Dilemma\n",
        "\n",
        "A one shot game. can either be a Pure Strategy: A player chooses a specific action with certaintyach player has made a definite choice without randomization. or a mixed strategy: A player assigns a probability to each possible action and then randomly selects an action based on these probabilities\n",
        "  >> In a one-shot game, players make their choices simultaneously and only once. There is no opportunity to adjust strategies based on the other player's past actions, as the game ends after this single round.\n",
        "Possible strategies in iterated versions:\n",
        "  -->Tit fot tat, grim trigger, always defect, alwas cooperate.\n"
      ],
      "metadata": {
        "id": "VeCo-rAB_M4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic setup\n",
        "\n",
        "# Define payoffs\n",
        "PAYOFFS = {\n",
        "    ('Cooperate', 'Cooperate'): (3, 3),\n",
        "    ('Cooperate', 'Defect'): (0, 5),\n",
        "    ('Defect', 'Cooperate'): (5, 0),\n",
        "    ('Defect', 'Defect'): (1, 1)\n",
        "}\n",
        "\n",
        "def play_game(player1_strategy, player2_strategy):\n",
        "    return PAYOFFS[(player1_strategy, player2_strategy)]\n",
        "\n",
        "# Example usage\n",
        "player1_choice = 'Cooperate'\n",
        "player2_choice = 'Defect'\n",
        "\n",
        "payoff = play_game(player1_choice, player2_choice)\n",
        "print(f\"Player 1 chooses {player1_choice}, Player 2 chooses {player2_choice}\")\n",
        "print(f\"Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n"
      ],
      "metadata": {
        "id": "1_T0Lh0hjiS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoo4TVfJ8V4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4618446e-b857-4a41-ff0f-ca4b300c80d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 2: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 3: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 4: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 5: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 6: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 7: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 8: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 9: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 10: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "\n",
            "Total Payoffs after 10 rounds: Player 1: 50, Player 2: 0\n"
          ]
        }
      ],
      "source": [
        "# introduce rounds with some randomness --> doesn't garantee a game like feeling\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "# Define payoffs\n",
        "PAYOFFS = {\n",
        "    ('Cooperate', 'Cooperate'): (3, 3),\n",
        "    ('Cooperate', 'Defect'): (0, 5),\n",
        "    ('Defect', 'Cooperate'): (5, 0),\n",
        "    ('Defect', 'Defect'): (1, 1)\n",
        "}\n",
        "\n",
        "def play_game(player1_strategy, player2_strategy, rounds):\n",
        "    if (player1_strategy, player2_strategy) not in PAYOFFS:\n",
        "        raise ValueError(\"Invalid strategy. Choose from 'Cooperate' or 'Defect'.\")\n",
        "\n",
        "    # Initialize cumulative payoffs\n",
        "    total_payoff_player1 = 0\n",
        "    total_payoff_player2 = 0\n",
        "\n",
        "    # Loop through the number of rounds\n",
        "    for current_round in range(1, rounds + 1):\n",
        "        payoff = PAYOFFS[(player1_strategy, player2_strategy)]\n",
        "\n",
        "        # Print round details\n",
        "        print(f\"Round {current_round}: Player 1 chooses {player1_strategy}, Player 2 chooses {player2_strategy}\")\n",
        "        print(f\"Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n",
        "\n",
        "        total_payoff_player1 += payoff[0]\n",
        "        total_payoff_player2 += payoff[1]\n",
        "\n",
        "    return total_payoff_player1, total_payoff_player2\n",
        "\n",
        "# Possible choices (tuples)\n",
        "possible_choices = list(PAYOFFS.keys())\n",
        "\n",
        "# Convert choices to a list of individual strategies\n",
        "strategies = list(set([choice for sublist in possible_choices for choice in sublist]))\n",
        "\n",
        "# Randomly select strategies\n",
        "player1_choice = choice(strategies)\n",
        "player2_choice = choice(strategies)\n",
        "\n",
        "# Rounds to be played\n",
        "rounds = 3\n",
        "\n",
        "# Play the game\n",
        "payoff = play_game(player1_choice, player2_choice, rounds)\n",
        "print(f\"\\nTotal Payoffs after {rounds} rounds: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### > Explore strategies"
      ],
      "metadata": {
        "id": "yaMf2FzZhfMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "# Define payoffs\n",
        "PAYOFFS = {\n",
        "    ('Cooperate', 'Cooperate'): (3, 3),\n",
        "    ('Cooperate', 'Defect'): (0, 5),\n",
        "    ('Defect', 'Cooperate'): (5, 0),\n",
        "    ('Defect', 'Defect'): (1, 1)\n",
        "}\n",
        "\n",
        "def explore_payoffs():\n",
        "    print(\"Exploring all possible payoffs:\")\n",
        "    for strategies, payoff in PAYOFFS.items():\n",
        "        print(f\"Strategies: {strategies} -> Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n",
        "    print()\n",
        "\n",
        "def play_game(player1_strategy, player2_strategy, rounds):\n",
        "    if (player1_strategy, player2_strategy) not in PAYOFFS:\n",
        "        raise ValueError(\"Invalid strategy. Choose from 'Cooperate' or 'Defect'.\")\n",
        "\n",
        "    total_payoff_player1 = 0\n",
        "    total_payoff_player2 = 0\n",
        "\n",
        "    for current_round in range(1, rounds + 1):\n",
        "        payoff = PAYOFFS[(player1_strategy, player2_strategy)]\n",
        "\n",
        "        print(f\"Round {current_round}: Player 1 chooses {player1_strategy}, Player 2 chooses {player2_strategy}\")\n",
        "        print(f\"Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n",
        "\n",
        "        total_payoff_player1 += payoff[0]\n",
        "        total_payoff_player2 += payoff[1]\n",
        "\n",
        "    return total_payoff_player1, total_payoff_player2\n",
        "\n",
        "# Explore all possible payoffs\n",
        "explore_payoffs()\n",
        "\n",
        "# Possible choices (tuples)\n",
        "possible_choices = list(PAYOFFS.keys())\n",
        "\n",
        "# Convert choices to a list of individual strategies\n",
        "strategies = list(set([choice for sublist in possible_choices for choice in sublist]))\n",
        "\n",
        "# Randomly select strategies for simulation\n",
        "player1_choice = choice(strategies)\n",
        "player2_choice = choice(strategies)\n",
        "\n",
        "# Rounds to be played\n",
        "rounds = 3\n",
        "\n",
        "# Play the game\n",
        "payoff = play_game(player1_choice, player2_choice, rounds)\n",
        "print(f\"\\nTotal Payoffs after {rounds} rounds: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M16yL6BIhlSW",
        "outputId": "d514d729-e1d4-4751-8810-33b7e6ba8610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring all possible payoffs:\n",
            "Strategies: ('Cooperate', 'Cooperate') -> Payoffs: Player 1: 3, Player 2: 3\n",
            "Strategies: ('Cooperate', 'Defect') -> Payoffs: Player 1: 0, Player 2: 5\n",
            "Strategies: ('Defect', 'Cooperate') -> Payoffs: Player 1: 5, Player 2: 0\n",
            "Strategies: ('Defect', 'Defect') -> Payoffs: Player 1: 1, Player 2: 1\n",
            "\n",
            "Round 1: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 2: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "Round 3: Player 1 chooses Defect, Player 2 chooses Cooperate\n",
            "Payoffs: Player 1: 5, Player 2: 0\n",
            "\n",
            "Total Payoffs after 3 rounds: Player 1: 15, Player 2: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I suppose now we can deduce that we require a more dynamice strategy and choice implementation. Here on we will review simple reinforcement learning (RL) and or Evolutionary Game Theory (EGT) methods."
      ],
      "metadata": {
        "id": "KNC8A73tjGcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EGT vs RL Application\n",
        "\n",
        "Evolutionary Game Theory and Reinforcement Learning are both approaches used to study and optimize decision-making in environments with strategic interactions, but they are grounded in different theoretical frameworks and are used for different purposes.\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "FFr9TZzgkSB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **EGT based dilemma**\n",
        "\n",
        "Implementation of EGT where strategies evolve based on their success..\n",
        "\n",
        "--\n",
        "\n",
        " ### Summary\n",
        "\n",
        "#### Initialization\n",
        "- **Population:** Begins with a randomly chosen set of strategies, either 'Cooperate' or 'Defect'.\n",
        "\n",
        "#### Fitness Evaluation\n",
        "- **play_game:** Simulates interactions between strategies and calculates the resulting payoffs.\n",
        "- **evolve_population:** Assesses the fitness of each strategy by evaluating its performance against all other strategies in the population.\n",
        "\n",
        "#### Strategy Selection\n",
        "- **Best Strategy:** The strategy with the highest fitness score is selected as the most successful.\n",
        "- **Mutation:** Introduces genetic diversity by randomly altering some strategies.\n",
        "\n",
        "#### Evolution Simulation\n",
        "- **Generations:** The population evolves over multiple generations. Strategies are updated based on their fitness and the mutation process.\n",
        "\n",
        "#### Results\n",
        "- **Population Evolution:** Displays the population of strategies for each generation, illustrating how strategies develop over time.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "biPgcQ_Dlbmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "# Define payoffs\n",
        "PAYOFFS = {\n",
        "    ('Cooperate', 'Cooperate'): (3, 3),\n",
        "    ('Cooperate', 'Defect'): (0, 5),\n",
        "    ('Defect', 'Cooperate'): (5, 0),\n",
        "    ('Defect', 'Defect'): (1, 1)\n",
        "}\n",
        "\n",
        "def explore_payoffs():\n",
        "    print(\"Exploring all possible payoffs:\")\n",
        "    for strategies, payoff in PAYOFFS.items():\n",
        "        print(f\"Strategies: {strategies} -> Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n",
        "    print()\n",
        "\n",
        "def play_game(player1_strategy, player2_strategy, rounds):\n",
        "    if (player1_strategy, player2_strategy) not in PAYOFFS:\n",
        "        raise ValueError(\"Invalid strategy. Choose from 'Cooperate' or 'Defect'.\")\n",
        "\n",
        "    total_payoff_player1 = 0\n",
        "    total_payoff_player2 = 0\n",
        "\n",
        "    for current_round in range(1, rounds + 1):\n",
        "        payoff = PAYOFFS[(player1_strategy, player2_strategy)]\n",
        "\n",
        "        total_payoff_player1 += payoff[0]\n",
        "        total_payoff_player2 += payoff[1]\n",
        "\n",
        "    return total_payoff_player1, total_payoff_player2\n",
        "\n",
        "def evolve_population(population, rounds, mutation_rate=0.1):\n",
        "    strategies = list(set([s for sublist in PAYOFFS.keys() for s in sublist]))\n",
        "    new_population = []\n",
        "\n",
        "    for individual in population:\n",
        "        # Evaluate fitness\n",
        "        fitness = {strategy: 0 for strategy in strategies}\n",
        "        for strategy in strategies:\n",
        "            total_payoff = 0\n",
        "            for opponent_strategy in strategies:\n",
        "                payoff = play_game(strategy, opponent_strategy, rounds)\n",
        "                total_payoff += payoff[0]\n",
        "            fitness[strategy] = total_payoff\n",
        "\n",
        "        # Select the best strategy based on fitness\n",
        "        best_strategy = max(fitness, key=fitness.get)\n",
        "\n",
        "        # Mutation: with some probability, change the strategy\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            best_strategy = np.random.choice(strategies)\n",
        "\n",
        "        new_population.append(best_strategy)\n",
        "\n",
        "    return new_population\n",
        "\n",
        "# Initialize population with random strategies\n",
        "initial_population_size = 10\n",
        "strategies = list(set([s for sublist in PAYOFFS.keys() for s in sublist]))\n",
        "population = [np.random.choice(strategies) for _ in range(initial_population_size)]\n",
        "\n",
        "# Simulate evolution\n",
        "rounds = 3\n",
        "generations = 5\n",
        "\n",
        "print(\"Initial population:\", population)\n",
        "for generation in range(generations):\n",
        "    population = evolve_population(population, rounds)\n",
        "    print(f\"Generation {generation + 1}: {population}\")\n",
        "\n",
        "# Explore all possible payoffs\n",
        "explore_payoffs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIqqkABRkGtP",
        "outputId": "a601bd00-2158-4ede-bb10-a85c8e4d569a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial population: ['Defect', 'Defect', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate', 'Defect']\n",
            "Generation 1: ['Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Generation 2: ['Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Generation 3: ['Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Generation 4: ['Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Generation 5: ['Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Exploring all possible payoffs:\n",
            "Strategies: ('Cooperate', 'Cooperate') -> Payoffs: Player 1: 3, Player 2: 3\n",
            "Strategies: ('Cooperate', 'Defect') -> Payoffs: Player 1: 0, Player 2: 5\n",
            "Strategies: ('Defect', 'Cooperate') -> Payoffs: Player 1: 5, Player 2: 0\n",
            "Strategies: ('Defect', 'Defect') -> Payoffs: Player 1: 1, Player 2: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tSuGJmtPnHsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **RL based dilemma**\n",
        "\n",
        "We enhance the agents' ability to learn and adapt their strategies based on interactions with the environment.\n",
        "\n",
        "By integrating RL into the EGT model:\n",
        "- Agents can continuously learn and optimize their strategies based on feedback\n",
        "\n",
        "- Evolve through generational changes.\n",
        "\n",
        "- Enhance the model's ability to simulate adaptive and competitive behavior in complex environments.\n",
        "---\n",
        "\n",
        "## Explanation of Q-Learning Integration into EGT Model\n",
        "\n",
        "### Q-Learning Integration\n",
        "\n",
        "- **Q-Values:**\n",
        "  - Track the estimated value of each strategy pair. Initially, all Q-values are set to zero.\n",
        "  - Q-values are updated based on the rewards received from interactions, reflecting the effectiveness of each strategy.\n",
        "\n",
        "- **q_learning_update Function:**\n",
        "  - This function updates the Q-value for a given strategy pair based on the received reward.\n",
        "  - The update rule typically involves adjusting the Q-value towards the observed reward, taking into account the learning rate and discount factor.\n",
        "\n",
        "### Policy for Strategy Selection\n",
        "\n",
        "- **Epsilon-Greedy:**\n",
        "  - **Exploration:** With probability epsilon (ε), strategies are chosen randomly to explore new possibilities.\n",
        "  - **Exploitation:** With probability 1-ε, strategies are selected based on the highest Q-value to exploit known successful strategies.\n",
        "  - This balance helps agents explore new strategies while optimizing based on learned experiences.\n",
        "\n",
        "### Evolution Process\n",
        "\n",
        "- **Fitness Calculation:**\n",
        "  - Assesses how well strategies perform in the environment, typically using accumulated payoffs.\n",
        "  - Fitness values guide the evolutionary process, determining which strategies are more successful.\n",
        "\n",
        "- **Strategy Update:**\n",
        "  - Strategies are adjusted using Q-learning based on their performance in interactions.\n",
        "  - The Q-learning algorithm updates strategy values, influencing future strategy choices.\n",
        "\n",
        "- **Mutation:**\n",
        "  - Introduces random changes to strategies to simulate genetic diversity.\n",
        "  - Mutation helps maintain diversity in the population and can lead to discovering new effective strategies.\n",
        "\n",
        "### Simulation\n",
        "\n",
        "- **Generations:**\n",
        "  - The population evolves over multiple generations.\n",
        "  - Strategies are continuously updated based on both the learning outcomes (from Q-learning) and evolutionary principles.\n",
        "  - The simulation tracks how strategies change over time, incorporating both learned and evolved adaptations.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IZGtY45mnIES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "# Define payoffs\n",
        "PAYOFFS = {\n",
        "    ('Cooperate', 'Cooperate'): (3, 3),\n",
        "    ('Cooperate', 'Defect'): (0, 5),\n",
        "    ('Defect', 'Cooperate'): (5, 0),\n",
        "    ('Defect', 'Defect'): (1, 1)\n",
        "}\n",
        "\n",
        "# Define Q-learning parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "gamma = 0.95  # Discount factor\n",
        "epsilon = 1  # Exploration rate\n",
        "\n",
        "# Initialize Q-values for strategies\n",
        "Q_values = {\n",
        "    ('Cooperate', 'Cooperate'): 0,\n",
        "    ('Cooperate', 'Defect'): 0,\n",
        "    ('Defect', 'Cooperate'): 0,\n",
        "    ('Defect', 'Defect'): 0\n",
        "}\n",
        "\n",
        "def explore_payoffs():\n",
        "    print(\"Exploring all possible payoffs:\")\n",
        "    for strategies, payoff in PAYOFFS.items():\n",
        "        print(f\"Strategies: {strategies} -> Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n",
        "    print()\n",
        "\n",
        "def play_game(player1_strategy, player2_strategy, rounds):\n",
        "    if (player1_strategy, player2_strategy) not in PAYOFFS:\n",
        "        raise ValueError(\"Invalid strategy. Choose from 'Cooperate' or 'Defect'.\")\n",
        "\n",
        "    total_payoff_player1 = 0\n",
        "    total_payoff_player2 = 0\n",
        "\n",
        "    for current_round in range(1, rounds + 1):\n",
        "        payoff = PAYOFFS[(player1_strategy, player2_strategy)]\n",
        "        total_payoff_player1 += payoff[0]\n",
        "        total_payoff_player2 += payoff[1]\n",
        "\n",
        "    return total_payoff_player1, total_payoff_player2\n",
        "\n",
        "def q_learning_update(strategy, opponent_strategy, reward):\n",
        "    global Q_values\n",
        "    # Update Q-value using the reward received\n",
        "    state_action_pair = (strategy, opponent_strategy)\n",
        "    best_next_action = max(Q_values, key=Q_values.get)\n",
        "    Q_values[state_action_pair] = Q_values[state_action_pair] + alpha * (reward + gamma * Q_values[best_next_action] - Q_values[state_action_pair])\n",
        "\n",
        "def evolve_population(population, rounds, mutation_rate=0.1):\n",
        "    strategies = list(set([s for sublist in PAYOFFS.keys() for s in sublist]))\n",
        "    new_population = []\n",
        "\n",
        "    for individual in population:\n",
        "        # Choose strategy using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            strategy = np.random.choice(strategies)\n",
        "        else:\n",
        "            strategy = max(Q_values, key=Q_values.get)[0]  # Choose best strategy based on Q-values\n",
        "\n",
        "        # Evaluate fitness\n",
        "        fitness = {strategy: 0 for strategy in strategies}\n",
        "        for opponent_strategy in strategies:\n",
        "            payoff = play_game(strategy, opponent_strategy, rounds)\n",
        "            fitness[strategy] += payoff[0]\n",
        "\n",
        "        # Select the best strategy based on fitness\n",
        "        best_strategy = max(fitness, key=fitness.get)\n",
        "\n",
        "        # Mutation: with some probability, change the strategy\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            best_strategy = np.random.choice(strategies)\n",
        "\n",
        "        # Update Q-values based on the chosen strategy\n",
        "        reward = fitness[best_strategy] / len(strategies)\n",
        "        q_learning_update(strategy, best_strategy, reward)\n",
        "\n",
        "        new_population.append(best_strategy)\n",
        "\n",
        "    return new_population\n",
        "\n",
        "# Initialize population with random strategies\n",
        "initial_population_size = 10\n",
        "strategies = list(set([s for sublist in PAYOFFS.keys() for s in sublist]))\n",
        "population = [np.random.choice(strategies) for _ in range(initial_population_size)]\n",
        "\n",
        "# Simulate evolution\n",
        "rounds = 10\n",
        "generations = 15\n",
        "\n",
        "print(\"Initial population:\", population)\n",
        "for generation in range(generations):\n",
        "    population = evolve_population(population, rounds)\n",
        "    print(f\"Generation {generation + 1}: {population}\")\n",
        "\n",
        "# Explore all possible payoffs\n",
        "explore_payoffs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS0Sc2kLpVuo",
        "outputId": "7dc63444-d420-4a5f-96a9-faf7f0ce92f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial population: ['Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Cooperate']\n",
            "Generation 1: ['Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Defect', 'Cooperate']\n",
            "Generation 2: ['Defect', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate', 'Defect', 'Defect']\n",
            "Generation 3: ['Defect', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Defect']\n",
            "Generation 4: ['Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect']\n",
            "Generation 5: ['Defect', 'Defect', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Defect', 'Cooperate', 'Defect']\n",
            "Generation 6: ['Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Cooperate']\n",
            "Generation 7: ['Defect', 'Cooperate', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Cooperate', 'Defect', 'Defect', 'Cooperate']\n",
            "Generation 8: ['Defect', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate', 'Defect', 'Defect', 'Cooperate', 'Cooperate']\n",
            "Generation 9: ['Cooperate', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Defect']\n",
            "Generation 10: ['Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Defect', 'Defect']\n",
            "Generation 11: ['Cooperate', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate']\n",
            "Generation 12: ['Cooperate', 'Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate']\n",
            "Generation 13: ['Cooperate', 'Cooperate', 'Cooperate', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Cooperate', 'Defect', 'Defect']\n",
            "Generation 14: ['Cooperate', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate', 'Cooperate', 'Cooperate']\n",
            "Generation 15: ['Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Cooperate', 'Defect', 'Cooperate', 'Cooperate']\n",
            "Exploring all possible payoffs:\n",
            "Strategies: ('Cooperate', 'Cooperate') -> Payoffs: Player 1: 3, Player 2: 3\n",
            "Strategies: ('Cooperate', 'Defect') -> Payoffs: Player 1: 0, Player 2: 5\n",
            "Strategies: ('Defect', 'Cooperate') -> Payoffs: Player 1: 5, Player 2: 0\n",
            "Strategies: ('Defect', 'Defect') -> Payoffs: Player 1: 1, Player 2: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can now define the success criteria!!!\n",
        "---\n",
        "\n",
        "- **Individual Winner:** The strategy that accumulates the most payoff in its interactions.\n",
        "\n",
        "- **Generation Winner:** The generation where the population contains the highest-performing strategy.\n",
        "\n",
        "- **Winning Criteria:** Defined as the strategy with the highest total payoff or fitness score.\n"
      ],
      "metadata": {
        "id": "dkDuhnN3raVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "# Define payoffs\n",
        "PAYOFFS = {\n",
        "    ('Cooperate', 'Cooperate'): (3, 3),\n",
        "    ('Cooperate', 'Defect'): (0, 5),\n",
        "    ('Defect', 'Cooperate'): (5, 0),\n",
        "    ('Defect', 'Defect'): (1, 1)\n",
        "}\n",
        "\n",
        "# Define Q-learning parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.01  # Exploration rate\n",
        "\n",
        "# Initialize Q-values for strategies\n",
        "Q_values = {\n",
        "    ('Cooperate', 'Cooperate'): 0,\n",
        "    ('Cooperate', 'Defect'): 0,\n",
        "    ('Defect', 'Cooperate'): 0,\n",
        "    ('Defect', 'Defect'): 0\n",
        "}\n",
        "\n",
        "def explore_payoffs():\n",
        "    print(\"Exploring all possible payoffs:\")\n",
        "    for strategies, payoff in PAYOFFS.items():\n",
        "        print(f\"Strategies: {strategies} -> Payoffs: Player 1: {payoff[0]}, Player 2: {payoff[1]}\")\n",
        "    print()\n",
        "\n",
        "def play_game(player1_strategy, player2_strategy, rounds):\n",
        "    if (player1_strategy, player2_strategy) not in PAYOFFS:\n",
        "        raise ValueError(\"Invalid strategy. Choose from 'Cooperate' or 'Defect'.\")\n",
        "\n",
        "    total_payoff_player1 = 0\n",
        "    total_payoff_player2 = 0\n",
        "\n",
        "    for current_round in range(1, rounds + 1):\n",
        "        payoff = PAYOFFS[(player1_strategy, player2_strategy)]\n",
        "        total_payoff_player1 += payoff[0]\n",
        "        total_payoff_player2 += payoff[1]\n",
        "\n",
        "    return total_payoff_player1, total_payoff_player2\n",
        "\n",
        "def q_learning_update(strategy, opponent_strategy, reward):\n",
        "    global Q_values\n",
        "    # Update Q-value using the reward received\n",
        "    state_action_pair = (strategy, opponent_strategy)\n",
        "    best_next_action = max(Q_values, key=Q_values.get)\n",
        "    Q_values[state_action_pair] = Q_values[state_action_pair] + alpha * (reward + gamma * Q_values[best_next_action] - Q_values[state_action_pair])\n",
        "\n",
        "def evolve_population(population, rounds, mutation_rate=0.1):\n",
        "    strategies = list(set([s for sublist in PAYOFFS.keys() for s in sublist]))\n",
        "    new_population = []\n",
        "\n",
        "    fitness_scores = {strategy: 0 for strategy in strategies}\n",
        "    for strategy in strategies:\n",
        "        total_payoff = 0\n",
        "        for opponent_strategy in strategies:\n",
        "            payoff = play_game(strategy, opponent_strategy, rounds)\n",
        "            total_payoff += payoff[0]\n",
        "        fitness_scores[strategy] = total_payoff\n",
        "\n",
        "    for individual in population:\n",
        "        # Choose strategy using epsilon-greedy policy\n",
        "        if np.random.rand() < epsilon:\n",
        "            strategy = np.random.choice(strategies)\n",
        "        else:\n",
        "            strategy = max(fitness_scores, key=fitness_scores.get)\n",
        "\n",
        "        # Update Q-values based on the chosen strategy\n",
        "        reward = fitness_scores[strategy] / len(strategies)\n",
        "        q_learning_update(strategy, max(fitness_scores, key=fitness_scores.get), reward)\n",
        "\n",
        "        # Mutation: with some probability, change the strategy\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            strategy = np.random.choice(strategies)\n",
        "\n",
        "        new_population.append(strategy)\n",
        "\n",
        "    return new_population\n",
        "\n",
        "def evaluate_population(population, rounds):\n",
        "    # Calculate total payoffs for each individual\n",
        "    payoffs = {strategy: 0 for strategy in set(population)}\n",
        "    for strategy in payoffs:\n",
        "        total_payoff = 0\n",
        "        for opponent_strategy in set(population):\n",
        "            payoff = play_game(strategy, opponent_strategy, rounds)\n",
        "            total_payoff += payoff[0]\n",
        "        payoffs[strategy] = total_payoff\n",
        "\n",
        "    # Determine the best strategy\n",
        "    best_strategy = max(payoffs, key=payoffs.get)\n",
        "    return best_strategy, payoffs[best_strategy]\n",
        "\n",
        "# Initialize population with random strategies\n",
        "initial_population_size = 10\n",
        "strategies = list(set([s for sublist in PAYOFFS.keys() for s in sublist]))\n",
        "population = [np.random.choice(strategies) for _ in range(initial_population_size)]\n",
        "\n",
        "# Simulate evolution\n",
        "rounds = 5\n",
        "generations = 5\n",
        "\n",
        "for generation in range(generations):\n",
        "    population = evolve_population(population, rounds)\n",
        "    best_strategy, best_payoff = evaluate_population(population, rounds)\n",
        "    print(f\"Generation {generation + 1}:\")\n",
        "    print(f\"Population: {population}\")\n",
        "    print(f\"Best Strategy: {best_strategy} with Payoff: {best_payoff}\")\n",
        "\n",
        "# Explore all possible payoffs\n",
        "explore_payoffs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtTnfAZNruZW",
        "outputId": "8c22c4f7-2374-401d-ddd0-caed875a3928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 1:\n",
            "Population: ['Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Best Strategy: Defect with Payoff: 5\n",
            "Generation 2:\n",
            "Population: ['Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Defect', 'Defect']\n",
            "Best Strategy: Defect with Payoff: 30\n",
            "Generation 3:\n",
            "Population: ['Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Best Strategy: Defect with Payoff: 5\n",
            "Generation 4:\n",
            "Population: ['Defect', 'Defect', 'Defect', 'Defect', 'Cooperate', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect']\n",
            "Best Strategy: Defect with Payoff: 30\n",
            "Generation 5:\n",
            "Population: ['Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Defect', 'Cooperate']\n",
            "Best Strategy: Defect with Payoff: 30\n",
            "Exploring all possible payoffs:\n",
            "Strategies: ('Cooperate', 'Cooperate') -> Payoffs: Player 1: 3, Player 2: 3\n",
            "Strategies: ('Cooperate', 'Defect') -> Payoffs: Player 1: 0, Player 2: 5\n",
            "Strategies: ('Defect', 'Cooperate') -> Payoffs: Player 1: 5, Player 2: 0\n",
            "Strategies: ('Defect', 'Defect') -> Payoffs: Player 1: 1, Player 2: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "----\n",
        "\n",
        "# Ending note: Lets Understand the Point of the Simulation\n",
        "\n",
        "1. **Exploring Strategic Interactions:**\n",
        "   - The notebook demonstrates how different strategies can be applied in a game theory scenario, particularly the Prisoner's Dilemma. The main point is to understand how strategies like \"Cooperate\" and \"Defect\" can lead to different outcomes based on the payoffs associated with each choice.\n",
        "\n",
        "2. **Nash Equilibrium Concept:**\n",
        "   - By running simulations, we explore the concept of Nash equilibrium—where no player can benefit from changing their strategy unilaterally, assuming the other player's strategy remains the same. This is crucial for understanding stable outcomes in strategic interactions.\n",
        "\n",
        "3. **Strategy Evolution and Adaptation:**\n",
        "   - The notebook uses evolutionary game theory and reinforcement learning to simulate how strategies evolve over time. It showcases:\n",
        "     - **Evolutionary Game Theory (EGT):** How strategies change based on their success in the population. The aim is to see which strategies become dominant over generations.\n",
        "     - **Reinforcement Learning (RL):** How agents can learn and adapt their strategies based on feedback and interactions, improving their performance over time.\n",
        "\n",
        "4. **Practical Applications:**\n",
        "   - These simulations provide insights into real-world scenarios where strategic decision-making is essential. For instance:\n",
        "     - **Economic Models:** Understanding competitive behaviors and market dynamics.\n",
        "     - **Social Interactions:** Analyzing cooperation and competition in social or organizational settings.\n",
        "     - **Algorithm Design:** Improving algorithms for optimization problems where decision-making under uncertainty is involved.\n",
        "\n",
        "5. **Visualization of Outcomes:**\n",
        "   - The notebook helps visualize the outcomes of different strategies and how they affect payoffs. It allows you to see how certain strategies (e.g., always defecting) can dominate others and how evolutionary processes influence strategy selection over time.\n",
        "\n",
        "**Summary:** The primary goal is to provide a practical understanding of game theory concepts through simulations. This includes how Nash equilibrium works, how strategies evolve and adapt, and how reinforcement learning can be used to enhance decision-making strategies.\n",
        "\n",
        "In essence, the point is to explore and visualize the dynamics of strategic interactions, strategy evolution, and learning, which have broad implications for various fields including economics, social sciences, and artificial intelligence.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3beYvqzeuL6v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BuEuqAHluYmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}